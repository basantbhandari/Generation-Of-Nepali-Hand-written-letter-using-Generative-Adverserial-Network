{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nepali_letter_gen_using_dcgan_on_nepali_letter_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Qu4nDZsGkR"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao7fJW1Pyiza"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDGiwkGJpojt"
      },
      "source": [
        "# !pip install torch-summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM_Nur7xsgLd"
      },
      "source": [
        "# **Importing necessary library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypvVr9GtqSVS"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn                                          # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim                                    # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms                    # Transformations we can perform on our dataset\n",
        "from torch.utils.data import (DataLoader,)                     # Gives easier dataset managment and creates mini batches\n",
        "from torch.utils.tensorboard import SummaryWriter              # to print to tensorboard\n",
        "import pickle                                                  # for importing dataset from pickle file\n",
        "import matplotlib.pyplot as plt                                # to plot images\n",
        "import datetime                                                # for taking the current data and time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5se7VxOQzHLE"
      },
      "source": [
        "# **Defining classs for convolutiona neural network as generator and discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYodJeVvzMN2"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # torch.nn.ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, \n",
        "            # padding: Union[T, Tuple[T, T]] = 0, output_padding: Union[T, Tuple[T, T]] = 0, groups: int = 1, bias: bool = True, dilation: int = 1, padding_mode: str = 'zeros')\n",
        "            # N x channels_img x 64 x 64\n",
        "            # (1, 16, 4, 2, 1)\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # N x features_d x 32 x 32\n",
        "            # (16, 32, 4, 2, 1)\n",
        "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_d * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # (32, 64, 4, 2, 1)\n",
        "            nn.Conv2d(features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_d * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # (64, 128, 4, 2, 1)\n",
        "            nn.Conv2d(features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_d * 8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # N x features_d*8 x 4 x 4\n",
        "            # (128, 1, 4, 2, 1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            # N x 1 x 1 x 1\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            # torch.nn.ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, \n",
        "            # padding: Union[T, Tuple[T, T]] = 0, output_padding: Union[T, Tuple[T, T]] = 0, groups: int = 1, bias: bool = True, dilation: int = 1, padding_mode: str = 'zeros')\n",
        "            # N x channels_noise x 1 x 1\n",
        "            # (256, 256, 4, 1, 0)\n",
        "            nn.ConvTranspose2d(channels_noise, features_g * 16, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(features_g * 16),\n",
        "            nn.ReLU(),\n",
        "            # N x features_g*16 x 4 x 4\n",
        "            # (256, 128, 4, 2, 1)\n",
        "            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_g * 8),\n",
        "            nn.ReLU(),\n",
        "            # (128, 64, 4, 2, 1)\n",
        "            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_g * 4),\n",
        "            nn.ReLU(),\n",
        "            # (64, 32, 4, 2, 1)\n",
        "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_g * 2),\n",
        "            nn.ReLU(),\n",
        "            # (32, 1, 4, 2, 1)\n",
        "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
        "            # N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DTkBTQNqRH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMl4CzYz0Y6"
      },
      "source": [
        "# **Defining the necessary parameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBiGjyUfsINn"
      },
      "source": [
        "\n",
        "# Hyperparameters\n",
        "lr = 0.0005\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "channels_img = 1\n",
        "channels_noise = 512\n",
        "num_epochs = 1\n",
        "number_pixel = 64\n",
        "\n",
        "# For how many channels Generator and Discriminator should use\n",
        "features_d = 16\n",
        "features_g = 16\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y2aeZJQ0Nks"
      },
      "source": [
        "# **Importing the required dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UV-CP6HtTVP"
      },
      "source": [
        "pickle_in = open(\"/content/drive/MyDrive/Colab Notebooks/pytorch/minor project/minor_project_sixth_result_final/Copy of X_desired_shape_resize_64_10000_each_class.pickle\",\"rb\")\n",
        "X = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvuJEBXD2cc5"
      },
      "source": [
        "# **Converting numpy array into tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4NMjz5gtjMQ"
      },
      "source": [
        "X = torch.Tensor(X)\n",
        "print(X[0].size())\n",
        "print(type(X))\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7KJnGDTtyJZ"
      },
      "source": [
        "# **Filtering dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2ZT-VtL2XLr"
      },
      "source": [
        "print(X[0].shape)\n",
        "print(X[:][:][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z_k1GTwtsWh"
      },
      "source": [
        "print(type(X))\n",
        "print(X.shape)\n",
        "print(X[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xnPMgelt5-M"
      },
      "source": [
        "for i in range(0, 460000,2000):\n",
        "  temp = X[i]\n",
        "  plt.imshow(temp[:][:].reshape([number_pixel,number_pixel]), cmap='gray')\n",
        "  if i== 6000:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxpRIaMB2rtl"
      },
      "source": [
        "X_new = X[0: 360000]\n",
        "print(X_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7FLWsBm3EjN"
      },
      "source": [
        "# **Feature scaling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v13vOubNt8ZK"
      },
      "source": [
        "def norm(x_r):\n",
        "    x_n = (2.0*x_r/255.0 -1)\n",
        "    return x_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xiYLvPH3H6S"
      },
      "source": [
        "def denorm(x_r):\n",
        "    x_n = 255.0 *(x_r + 1) / 2.0\n",
        "    return x_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOQJ_k9e3IQ1"
      },
      "source": [
        "for i in range(0, 360000):\n",
        "  X_new[i] =norm(X_new[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HCUI9JsuMEG"
      },
      "source": [
        "X_new[0,:, 10:15, 10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tzriA-n3H-T"
      },
      "source": [
        "torch.min(X_new[0]), torch.max(X_new[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8JCGM6d3H2w"
      },
      "source": [
        "img_norm = denorm(X_new[0])\n",
        "print(img_norm.shape)\n",
        "print(img_norm[10:15, 10:15])\n",
        "plt.imshow(img_norm[:][:].reshape([number_pixel,number_pixel]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8312B_lBN0O6"
      },
      "source": [
        "dataloader = DataLoader(X_new, batch_size=batch_size, shuffle=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYDWJgkYSV5B"
      },
      "source": [
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "lossG_real_for_plot = []\n",
        "lossD_real_for_plot = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDHBiv7obo3y"
      },
      "source": [
        "# Create discriminator and generator\n",
        "netD = Discriminator(channels_img, features_d).to(device)\n",
        "netG = Generator(channels_noise, channels_img, features_g).to(device)\n",
        "\n",
        "# Setup Optimizer for G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "netG.train()\n",
        "netD.train()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "fixed_noise = torch.randn(64, channels_noise, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"runs/GAN_NEPALI/test_real\")\n",
        "writer_fake = SummaryWriter(f\"runs/GAN_NEPALI/test_fake\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPM9yWmU3tEB"
      },
      "source": [
        "CHECKPOINT_FILENAME = \"/content/drive/MyDrive/Colab Notebooks/pytorch/minor project/minor_project_sixth_result_final/checkPoint_holder/my_checkpoint.pth.tar\"\n",
        "def save_checkpoint(state,filename=CHECKPOINT_FILENAME):\n",
        "  print(\"********************************Saving check point ******************************\")\n",
        "  torch.save(state,filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv1rhg1G-LxY"
      },
      "source": [
        "def load_checkpoint(checkpoint):\n",
        "  print(\"..............Loading the saved checkpoints...........\")\n",
        "  netG.load_state_dict(checkpoint['state_dict_g'])\n",
        "  optimizerG.load_state_dict(checkpoint['optimizer_g'])\n",
        "  lossG = checkpoint['loss_g']\n",
        "  lossD = checkpoint['loss_d']\n",
        "  D_x = checkpoint['d_out']\n",
        "  netG.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh3kvyc8kMUl"
      },
      "source": [
        "\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data) in enumerate(dataloader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        ### Train Discriminator: max log(1 - D(G(z)))\n",
        "        netD.zero_grad()\n",
        "        label = (torch.ones(batch_size) * 0.9).to(device)\n",
        "        output = netD(data).reshape(-1)\n",
        "        lossD_real = criterion(output, label)\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)\n",
        "        fake = netG(noise)\n",
        "        label = (torch.ones(batch_size) * 0.1).to(device)\n",
        "\n",
        "        output = netD(fake.detach()).reshape(-1)\n",
        "        lossD_fake = criterion(output, label)\n",
        "\n",
        "        lossD = lossD_real + lossD_fake\n",
        "        lossD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "\n",
        "        ### Train Generator: max log(D(G(z)))\n",
        "        netG.zero_grad()\n",
        "        label = torch.ones(batch_size).to(device)\n",
        "        output = netD(fake).reshape(-1)\n",
        "\n",
        "        lossG = criterion(output, label)\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "\n",
        "\n",
        "        # Print losses ocassionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            step += 1\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\Loss D: {lossD:.4f}, loss G: {lossG:.4f} D(x): {D_x:.4f}\")\n",
        "            SummaryWriter('runs/gen_loss').add_scalar('generator loss with epoch', lossG/100.0, (epoch*len(dataloader) + batch_idx))\n",
        "            SummaryWriter('runs/dis_loss').add_scalar('discriminator loss with epoch', lossD/100.0, epoch*len(dataloader) + batch_idx)\n",
        "\n",
        "            # # to save the model\n",
        "            # checkpoint = {\n",
        "            #               'state_dict_g': netG.state_dict(),\n",
        "            #               'optimizer_g': optimizerG.state_dict(),\n",
        "            #               'loss_g': lossG,\n",
        "            #               'loss_d': lossD,\n",
        "            #               'd_out': D_x\n",
        "                          \n",
        "            #               }\n",
        "            # save_checkpoint(checkpoint)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Save Losses for plotting later\n",
        "            G_losses.append(lossG.item())\n",
        "            D_losses.append(lossD.item())\n",
        "\n",
        "            lossG_real_for_plot.append(lossD_real.item())\n",
        "            lossD_real_for_plot.append(lossD_fake.item())\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise)\n",
        "                # torchvision.utils.make_grid(tensor: Union[torch.Tensor, List[torch.Tensor]], nrow: int = 8, padding: int = 2, normalize: bool = False, range: Optional[Tuple[int, int]] = None, scale_each: bool = False, pad_value: int = 0) â†’ torch.Tensor\n",
        "                img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "                writer_real.add_image(\"Nepali letter Real Images\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Nepali letter Fake Images\", img_grid_fake, global_step=step)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itZUlKA9seGP"
      },
      "source": [
        "print(netD)\n",
        "print(netG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36NrRsA6VuE6"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHbSKfTG3i9_"
      },
      "source": [
        "# plt.figure(figsize=(10,5))\n",
        "# plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "# plt.plot(lossG_real_for_plot,label=\"G_real_score\")\n",
        "# plt.plot(lossD_real_for_plot,label=\"D_fake_score\")\n",
        "# plt.xlabel(\"iterations\")\n",
        "# plt.ylabel(\"score\")\n",
        "# plt.legend()\n",
        "# plt.title('Score in GAN');\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slRzQmNpWF51"
      },
      "source": [
        "# **Loading the saved model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06avYaV4-RcH"
      },
      "source": [
        "load_checkpoint(torch.load(CHECKPOINT_FILENAME))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFcWog6S94BR"
      },
      "source": [
        "fake = netG(fixed_noise)\n",
        "print(fake.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d11vnJFY-S2O"
      },
      "source": [
        "print(fake[0][0].shape)\n",
        "print(type(fake[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlwNEVl-I_v"
      },
      "source": [
        "plt.imshow(fake[20][0].cpu().data.numpy(),  cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jy-hncA4hnW"
      },
      "source": [
        "%tensorboard --logdir runs/GAN_NEPALI"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}